\documentclass[12pt]{amsart}
\usepackage[margin = 1in]{geometry}
\usepackage{amsmath, amssymb, amsthm, graphicx}
\usepackage{enumerate}
\usepackage[all]{xy}
\usepackage{mathrsfs}

%%%%%%%%%%%%%% COLOR COMMENTS! %%%%%%%%%%%%%%%
\usepackage{color}
\newcommand{\dzb}[1]{{\color{blue} \sf
    $\spadesuit\spadesuit\spadesuit$ DZB: [#1]}}
\newcommand{\reword}[1]{{\color{red} \sf $\spadesuit\spadesuit\spadesuit$ reword: [#1]}}

% changed above definition to make comments disappear
%\newcommand{dzb}[1]{}
%\newcommand{reword}[1]{}

% Color names: http://en.wikibooks.org/wiki/LaTeX/Colors#The_68_standard_colors_known_to_dvips
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\note}[1]{{\color{BurntOrange} $\blacktriangle\blacktriangle$\sf Note: [#1]}}

\newcommand{\F}[0]{\mathbb{F}}
\newcommand{\tr}[0]{\operatorname{tr}}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\frob}[0]{\operatorname{Frob}}
\newcommand{\card}[0]{\#}
\newcommand{\pmat}[4]{\begin{pmatrix}#1 & #2 \\ #3 & #4\end{pmatrix}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\Ov}{\mathcal{O}_{v}}
\newcommand{\Ok}{\mathcal{O}_{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\leg}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\frakp}{\mathfrak{p}}
\newcommand{\frakq}{\mathfrak{q}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\nm}{\operatorname{N}_{K/\Q}}
\newcommand{\Cl}{\operatorname{Cl}}
\newcommand{\oo}{\mathcal{O}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\Gal}{\operatorname{Gal}}
\newcommand{\cl}{\overline}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\raym}{\mathcal{C}^{\frakm}_K}
\newcommand{\frakr}{\mathfrak{r}}
\newcommand{\Spec}{\text{Spec} \ }
\newcommand{\Hom}{\text{Hom}}
\newcommand{\ord}{\text{ord}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\Nm}{\text{Nm}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\ti}{\times}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\mbf}[1]{\mathbf{#1}}
%\newcommand{\leg}[2]{\left(\frac{ \# 1}{\# 2} \right)}

\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\Frob}{Frob}
\DeclareMathOperator{\ab}{ab}
\DeclareMathOperator{\cyc}{cyc}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Ver}{Ver}
\DeclareMathOperator{\Art}{Art}
\DeclareMathOperator{\Spl}{Spl}
\DeclareMathOperator{\sep}{sep}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Sp}{Sp}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\SU}{SU}
\DeclareMathOperator{\PGL}{PGL}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\WPr}{WPr}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ex}[thm]{Example}
\newtheorem{crit}[thm]{Criterion}


\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

\makeatletter
\def\imod#1{\allowbreak\mkern5mu({\operator@font mod}\,\,#1)}
\makeatother

\widowpenalty=1000
\clubpenalty=1000

\title{CS 181 Problem Set 2}
\author{Ashok Cutkosky and Tony Feng}


\begin{document}
\maketitle

\noindent \textbf{Problem 1.} It will be useful to note the following property of perceptrons, which will aid us in proving that certain problems are not linearly separable. By including a ``dummy feature'' $x_0$ which always has value $1$, we may assume that hypothesis function is given by 
\[
h_{\mbf{w}}(\mbf{x}) = g(\mbf{x}^T \mbf{w})
\]
Then if $\mbf{x}$ and $\mbf{y}$ are both classified positively, we have $h_{\mbf{w}}(\mbf{x}) >0 $ and $h_{\mbf{w}}(\mbf{y}) > 0$ so $h_{\mbf{w}}(\mbf{x}+\mbf{y}) > 0$ by linearity. 

If problems $\mbf{x}'$ and $\mbf{y}'$ that are both classified negatively, similar logic implies that $h_{\mbf{w}}(\mbf{x}'+\mbf{y}') \leq 0$. We have proved the following: 

\begin{lemma}\label{no_perceptron}
If there exist feature vectors $\mbf{x}, \mbf{y}$ that are classified positively and $\mbf{x}', \mbf{y}'$ that are classified negatively such that $\mbf{x} + \mbf{y} = \mbf{x}' + \mbf{y}'$, then the problem does not admit a solution by a perceptron. 
\end{lemma}

\begin{enumerate}
\item No perceptron can solve this. Indeed, take 
\[
\mbf{x} \leftrightarrow \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{pmatrix} \hspace{1cm} \mbf{y} \leftrightarrow \begin{pmatrix} -1 & -1 & -1 \\ -1 & -1 & -1 \\ -1 & -1 & -1 \end{pmatrix} 
\]
 and  
\[
\mbf{x} \leftrightarrow \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ -1 & -1 & -1 \end{pmatrix} \hspace{1cm} \mbf{y} \leftrightarrow \begin{pmatrix} -1 & -1 & -1 \\ -1 & -1 & -1 \\ 1 & 1 & 1 \end{pmatrix} ,
\]
which fulfill the hypotheses of Lemma \ref{no_perceptron}.

\item We claim that the perceptron with weights indicated by the matrix
\[
\begin{pmatrix} w_{11} = 1 & w_{12} = 1 & w_{13} = 1 \\ w_{21} = \frac{1}{2} & w_{22} = \frac{1}{2} &  w_{23} = \frac{1}{2}  \\ w_{31} = \frac{1}{2} & w_{32} = \frac{1}{2} & w_{33} = \frac{1}{2}  \end{pmatrix}
\]
classifies this problem. Indeed, suppose that there are $a$ ``on'' pixels in the top row and $b$ ``on'' pixes in the bottom two rows. Then $\mbf{w}^T  x = a - (3-a) + \frac{1}{2} (b - (3-b)) = 2a-3  - (b-3) = 2a - b$, which is positive if and only if $2a > b$. On the other hand, the fraction of ``on'' pixels in the top row is $\frac{a}{3}$ and the fraction of ``on'' pixels in the bottom two rows is $\frac{b}{6}$, so the former is larger than the latter if and only if $2a > b$, as desired. 

\item No perceptron can solve this. Indeed, take 
\[
\mbf{x} \leftrightarrow \begin{pmatrix} 1 & -1 & 1 \\ -1 & 1 & -1 \\ 1 & -1 & 1 \end{pmatrix} \hspace{1cm} \mbf{y} \leftrightarrow \begin{pmatrix} -1 & 1 & -1 \\ 1 & -1 & 1 \\ -1 & 1 & -1 \end{pmatrix} 
\]
 and  
\[
\mbf{x} \leftrightarrow \begin{pmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ -1 & -1 & -1 \end{pmatrix} \hspace{1cm} \mbf{y} \leftrightarrow \begin{pmatrix} -1 & -1 & -1 \\ -1 & -1 & -1 \\ 1 & 1 & 1 \end{pmatrix} ,
\]
which fulfill the hypotheses of Lemma \ref{no_perceptron}.
\end{enumerate}



\noindent \textbf{Problem 2.} 
\begin{enumerate}
\item \emph{Decision Trees} are effective when there are a few inputs of great significance. They can represent interesting interactions between inputs, which is useful in identifying idiosyncratic features of certain digits, such as contiguous segments.

However, decision trees are not effective at judging the input holistically, being required to focus on one feature at a time. This seems like a severe problem in digit recognition, where it is the general shape of the figure, rather than the particular input pixels that it occupies, which is important for identification. 

\item \emph{Boosted decision stumps} have the advantage of tending not to overfit, which seems like a real pitfall in the digit classification problem. Digit classification is qualitatively similar to polynomial fitting of data points, so it is conceivable that a learner would be susceptible to overfitting the specific features of the training data. 

However, boosting is essentially a hodgepodge of weak methods, and its effectiveness does depend on the efficacy of these weak learners. We have seen that naive learners struggle to identify features such as connectivity, which is certainly important in digit recognition. 

\item \emph{Perceptrons.} Perceptrons are useful for identifying linear decision boundaries, and can depend on all of the input data. In this way, they are better for holistically judging the input than are decision trees. 

However, they suffer from not being able to identify interesting interactions between the inputs, and such interactions are important in handwriting recognition for the purpose of identifying contiguous segments and loops. 

\item \emph{Multi-layer feed forward Neural Networks} possess much more flexibility and range than perceptrons. They can represent virtually any function of the input, given sufficiently many layers. 

However, the range of expression also makes neural networks vulnerable to overfitting. Regularization, pruning, and other methods should be used to ensure that this does not happen. 
\end{enumerate}

\noindent \textbf{Problem 3.}

\begin{itemize}
\item[1.-4.] See source code.

\item[5.]
Technically normalization is not absolutely necessary - the same algorithm will work, and there is a clear correspondence between normalized and non-normalized neural networks given by scaling the weights on edges coming out of the inputs on a non-normalized network by $256$ to get an equivalent normalized network. However normalization causes smaller changes in the weights to have larger effects on the outputs since the inputs are small and this may cause the network to learn somewhat faster.
\item[6.]
\begin{itemize}
\item[a.] Here are some plots of error for 10 epochs with various learning rates
\[
\begin{array}{cc}
\includegraphics[height=1.5in]{test_1}&\includegraphics[height=1.5in]{test_05}\\
\text{learning rate = 1.0}&\text{learning rate = 0.5}\\

\includegraphics[height=1.5in]{test_01}&\includegraphics[height=1.5in]{test_005}\\
\text{learning rate = 0.1}&\text{learning rate = 0.05}\\
\includegraphics[height=1.5in]{test_001}&\includegraphics[height=1.5in]{test_0001}\\
\text{learning rate = 0.01}&\text{learning rate = 0.001}
\end{array}
\]
We chose learning rate $0.05$
\item[b.] 
Here is a plot of error versus epochs

\includegraphics[height =2.5in]{simple100_005}

60 is a good number of epochs to train for, since validation set error stops decreasing at that point. It is important to use the validation set to tune the number of epochs because the validation set will measure how well our learning on the test set generalizes to other examples, so that we avoid overfitting.

\item[c.] At 60 epochs the training set performance was 0.863 and the validation set performance was 0.858 and the test performance was 0.916
\end{itemize}

\item[7.]
\begin{itemize}
\item[a.] Here are some plots for various learning rates for 15 nodes:
\[
\begin{array}{cc}
\includegraphics[height=1.5in]{hidden15test_1}&\includegraphics[height=1.5in]{hidden15test_075}\\
\text{learning rate = 1.0}&\text{learning rate = 0.75}\\
\includegraphics[height=1.5in]{hidden15test_05}&\includegraphics[height=1.5in]{hidden15test_125}\\
\text{learning rate = 0.5}&\text{learning rate = 1.25}\\
&\includegraphics[height=1.5in]{hidden15test_01}\\
&\text{learning rate = 0.1}
\end{array}
\]
The learning rate for 15 nodes was $0.5$.

Here are similar plots for 30 hidden nodes:
\[
\begin{array}{cc}
\includegraphics[height=1.5in]{hidden30test_1}&\includegraphics[height=1.5in]{hidden30test_075}\\
\text{learning rate = 1.0}&\text{learning rate = 0.75}\\
\includegraphics[height=1.5in]{hidden30test_05}&\includegraphics[height=1.5in]{hidden30test_125}\\
\text{learning rate = 0.5}&\text{learning rate = 1.25}\\
\end{array}
\]
The learning rate for 30 nodes we chose was $0.25$.

%stopped after 26 for 30.
%Test Performance:  0.946
%Validation Performance:  0.885
%Training Performance:  0.927444444444


%stopped after 20 for 15
%Test Performance:  0.933
%Validation Performance:  0.884
%Training Performance:  0.929666666667



\item[b.] We want to stop learning when the validation set performance levels off, since at this point we are starting to overfit the training set.
A rough proxy for this is to check whether the validation set performance at epoch $n$ is smaller  than the validation set performance at epoch $n-10$.
We can stop when this occurs.

\item[c.]
Training stopped after 20 epochs for 15 hidden nodes, and after 26 for 30 hidden nodes.
The plot for 15 hidden nodes is;

\includegraphics[height=2.5in]{hidden15_perf}

The plot for 30 hidden nodes is:

\includegraphics[height=2.5in]{hidden30_perf}

We choose 30 hidden nodes since it has the best validation performance.

\item[d.] The test performance of 30 hidden nodes was $0.946$, which is somewhat better than the simple perceptron test performance.

\end{itemize}

\item[8.] Our custom network features two hidden layers. The first hidden layer consists of nodes connected 
to connected $3 \times 3$ squares of the image, jumping by intervals of $3$ (so that the squares are 
disconnected). This idea exploits the locality of images - that is, the fact that we should focus on important 
\emph{groups} of pixels rather than individual pixels. It is one of the two important tools from the Le Cun et 
al. paper. The point is that we believe generalizable models should operate at a lower level of resolution. W

The second hidden layer is the same as in the simple neural network. e tried tuning parameters, such as the number of $3 \times 3$ squares examind and the number of hidden nodes in the second network. We observed that performance peaked 
at about $0.75$, so it was actually worse than the simple neural network. Perhaps our hypothesis about 
locality was wrong, or requires additional finessing to be effective. 


\begin{tabular}{|lll|}
\hline
Epoch & Performance (Training) & Performance (Validation) \\
\hline
1 & 0.10177778 & 0.114 \\
2 & 0.59233333 & 0.571 \\
3 & 0.70577778 & 0.679 \\
4 & 0.74166667 & 0.733 \\
5 & 0.73555556 & 0.750 \\
6 &  0.77600000 & 0.777 \\
7 & 0.73277778 & 0.722 \\
8 & 0.77411111 & 0.757 \\
9 & 0.76500000 & 0.748 \\
10 & 0.78688889 & 0.774 \\
\hline 
\end{tabular}
\end{itemize}


\noindent \textbf{Problem 4.}

\begin{itemize}

\item[1.]
$C$ is attempting to address the complexity of the network - it is implementing regularization. A network in which many weights are actually zero represents a graph in which some nodes in one layer are actually not connected to nodes in the next layer, which is a simpler scenario than the fully-connected network. The additional summation in $C$ penalizes higher weights so that the network will limit its complexity.

\item[2.]
If our learning rate is $\alpha$, then according to gradient descent, we want the update rule to be 
\[
w_{mj}^{(r+1)} \leftarrow w_{mj}^{(r)} + \alpha \frac{\partial C}{\partial w_{mj}^{(r)}}
\]
Now we have
\[
C=\mathcal{L}+\lambda \sum {w_{mj}}^2
\]
Where the second sum runs over all the weights in the network.
Thus
\[
\frac{\partial C}{\ w_{mj}} = \frac{\partial \mathcal{L}}{\partial w_{mj}}+2\lambda w_{mj}
\]
and 
\[
\frac{\partial C}{\ w_{km}} = \frac{\partial \mathcal{L}}{\partial w_{km}}+2\lambda w_{km}
\]
We already know $\frac{\partial \mathcal{L}}{\partial w_i}$ from the original backprogagation rule. Thus our update rule is
\[
w_{mj}^{(r+1)} \leftarrow w_{mj}^{(r)}+\alpha a_{m}\delta_j-2\alpha\lambda w_{mj}
\]
and analogous for $w_{km}$, just replacing the indices. 
\end{itemize}


\end{document}



