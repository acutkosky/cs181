\documentclass[12pt]{amsart}
\usepackage[margin = 1in]{geometry}
\usepackage{amsmath, amssymb, amsthm, graphicx}
\usepackage{enumerate}
\usepackage[all]{xy}
\usepackage{mathrsfs}

%%%%%%%%%%%%%% COLOR COMMENTS! %%%%%%%%%%%%%%%
\usepackage{color}
\newcommand{\dzb}[1]{{\color{blue} \sf
    $\spadesuit\spadesuit\spadesuit$ DZB: [#1]}}
\newcommand{\reword}[1]{{\color{red} \sf $\spadesuit\spadesuit\spadesuit$ reword: [#1]}}

% changed above definition to make comments disappear
%\newcommand{dzb}[1]{}
%\newcommand{reword}[1]{}

% Color names: http://en.wikibooks.org/wiki/LaTeX/Colors#The_68_standard_colors_known_to_dvips
\usepackage[usenames,dvipsnames]{xcolor}
\newcommand{\note}[1]{{\color{BurntOrange} $\blacktriangle\blacktriangle$\sf Note: [#1]}}

\newcommand{\F}[0]{\mathbb{F}}
\newcommand{\tr}[0]{\operatorname{tr}}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\frob}[0]{\operatorname{Frob}}
\newcommand{\card}[0]{\#}
\newcommand{\pmat}[4]{\begin{pmatrix}#1 & #2 \\ #3 & #4\end{pmatrix}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\Ov}{\mathcal{O}_{v}}
\newcommand{\Ok}{\mathcal{O}_{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\leg}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\frakp}{\mathfrak{p}}
\newcommand{\frakq}{\mathfrak{q}}
\newcommand{\mf}[1]{\mathfrak{#1}}
\newcommand{\nm}{\operatorname{N}_{K/\Q}}
\newcommand{\Cl}{\operatorname{Cl}}
\newcommand{\oo}{\mathcal{O}}
\newcommand{\sgn}{\operatorname{sgn}}
\newcommand{\Gal}{\operatorname{Gal}}
\newcommand{\cl}{\overline}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\raym}{\mathcal{C}^{\frakm}_K}
\newcommand{\frakr}{\mathfrak{r}}
\newcommand{\Spec}{\text{Spec} \ }
\newcommand{\Hom}{\text{Hom}}
\newcommand{\ord}{\text{ord}}
\newcommand{\scr}[1]{\mathscr{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cal}[1]{\mathcal{#1}}
\newcommand{\Nm}{\text{Nm}}
\newcommand{\A}{\mathbb{A}}
\newcommand{\ti}{\times}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\ul}[1]{\underline{#1}}
%\newcommand{\leg}[2]{\left(\frac{ \# 1}{\# 2} \right)}

\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\Frob}{Frob}
\DeclareMathOperator{\ab}{ab}
\DeclareMathOperator{\cyc}{cyc}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Ver}{Ver}
\DeclareMathOperator{\Art}{Art}
\DeclareMathOperator{\Spl}{Spl}
\DeclareMathOperator{\sep}{sep}
\DeclareMathOperator{\Stab}{Stab}
\DeclareMathOperator{\Sp}{Sp}
\DeclareMathOperator{\SO}{SO}
\DeclareMathOperator{\SU}{SU}
\DeclareMathOperator{\PGL}{PGL}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\argmax}{argmax}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ex}[thm]{Example}
\newtheorem{crit}[thm]{Criterion}


\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

\makeatletter
\def\imod#1{\allowbreak\mkern5mu({\operator@font mod}\,\,#1)}
\makeatother

\widowpenalty=1000
\clubpenalty=1000

\title{CS181 Assignment 3}
\author{Ashok Cutkosky and Tony Feng}


\begin{document}
\maketitle

\noindent \textbf{Problem 1.}
\begin{enumerate}

\item[a.]

This is the probability that $y$ lies in a box of sidelength $2\epsilon$ centered at $x$. This is just the ratio of the area of this box to the are of the whole box: $p=(2\epsilon)^M$.


\item[b.]
This probability is the ratio of the volume of the intersection of a box of sidelength $2\epsilon$ centered at $x$ with the box of sidelength 1 with the volume of the box of sidelength 1. The volume of this intersection is at most the volume of the box of sidength $2\epsilon$, so the ratio is at most $(2\epsilon)^M=p$.

\item[c.]
\[
||x-y||=\sqrt{\sum (x_i-y_i)^2}\ge \sqrt{\max (x_i-y_i)^2}=\max |x_i-y_i|
\]
so that $||x-y||\ge \max |x_i-y_i|$. The probability of being within distance $\epsilon$ of $x$ is the volume of the ball of radius $\epsilon$ about $x$. Since distance under this euclidean metric is at most distance under the previous metric, this ball has volume at most the volume of the ball under the previous metric, which is at most $p$.

\item[d.]
Let $p$ be the probability that for a given point $x$, a randomly drawn point $y$ will be within $\epsilon$ of $x$. Then the probability that for a given point $x$ a set of $N$ independently randomly drawn points $y$ will all NOT be within $\epsilon$ of $x$ is $(1-p)^N$. Thus the probability that the nearest neighbor of $x$ is within $\epsilon$ of $x$ is $1-(1-p)^N$.

Now $p$ is just the intersection of the ball of radius $\epsilon$ about $x$ and the box of sidelength $1$. The ball of radius $\epsilon$ about $x$ contains all the points of distance $\epsilon$ from $x$ along each of the $m$ principle axes, and so also contains the polyhedron given by the convex closure of these points. An $M$-dimensional pyramid with base area $A$ and height $h$ has volume $Ah/M$. Thus if $A_M$ is the volume of this polyhedron in $M$-dimensions, we have $A_M=2\epsilon A_{M-1}/M$, with $A_1=2\epsilon$. Thus $A_M=2^M\epsilon^M/M!$.

The volume of the intersection of the ball of radius $\epsilon$ and the box is minimized when $x$ is in a corner of the box, in which case the volume is given by the volume of the ball of radius $\epsilon$ divided by $2^M$. Thus $p \ge \epsilon^M/M!$. Thus the probability that a nearest neighbor of $x$ is witnih $\epsilon$ of $x$ is $1-(1-p)^N\ge 1-(1-\epsilon^M/M!)^N$ and so $N\ge \log \delta/\log(1-\epsilon^M/M!)$.

\item[e.]
Note that $\log(1-\epsilon^M/M!)$ approaches 0 very rapidly as $M$ increases so that the lower bound on $N$ increases very rapidly. Intuitively, this says that points become very far apart very quickly as the dimension increases and so HAC, which relies on distances between points, might become less effective as all these distances become very large.

\end{enumerate}
\noindent \textbf{Problem 2.}

\begin{enumerate}
\item[a.]
Let $\theta_{ML} = \argmax_{\theta}(Pr(D|\theta))$, $\theta_{MAP}=\argmax_{\theta}(Pr(D|\theta)Pr(\theta))$. Also, assign
\[
Pr(\theta|D) = \frac{Pr(D|\theta)Pr(\theta)}{\int_{\Theta} Pr(D\theta)Pr(\theta)d\theta}
\]
Then
\begin{align*}
Pr_{ML}(x|D) =& Pr(x|\theta_{ML}\\
Pr_{MAP}(x|D) = & Pr(x|\theta_{MAP})\\
Pr_{FB}(x|D) = & \int_{\Theta} Pr(x|\theta)Pr(\theta|D) d\theta
\end{align*}


\item[b.]
The MAP method can be considered more basian because it makes more use of priors.

\item[c.]
MAP enjoys the advantage of incorporating more knowledge about the world than the ML approach so that it is more resistant to overfitting. MAP enjoys the advantage of being computationally more tractable than the FB approach.

\item[d.]

\item[e.]
The Beta distributio is useful because it is a conjugate distribution - $p(\theta)$ has the same functional form as $p(\theta|x)$. This causes $p(\theta|x)$ to have a very simple form.

\item[f.]
In Fall 2011, the Harvad football team won 9 games and lost once. Using the the ML model, the probability of the team winning any particular game is then just $\frac{9}{10}$, and so the probability of winning the next game is also $\frac{9}{10}$.

Now we calculate using the MAP model. Let's adopt the $\beta(5,3)$ prior. Then following the lecture notes, 
\[
\argmax_{\theta}P(\theta|D) = \frac{5+9-1}{3+5+10-1}=\frac{13}{17}
\]
so that the probability of winning the next game is $\frac{13}{17}$.

Now we use the FB model, again with the $\beta(5,3)$ prior. Then we have
\[
\argmax_{\theta}P(\theta|D) = \int_{0}^1 \theta p(\theta|D) d\theta=\frac{5+9-1}{3+5+10-2}=\frac{13}{16}
\]
so that the probability of winning the next game is $\frac{13}{17}$.
\end{enumerate}
\noindent \textbf{Problem 5.}
\begin{enumerate}
\item[a.]
The loss function for vectors $x_1,\cdots,x_N$ with prototypes $y_1,\cdots,y_K$ and responsibilities $r_{ij}$ is
\[
\mathcal{L} = \sum_{n=1}^N \sum_{k=1}^K r_{nk}|x_n-y_k|
\]
Suppose we are in $M$-dimensional space, with $x_n=(x_{n,1},\cdots,x_{n,m})$ and $y_k=(y_{k,1},\cdots,y_{k,m})$. Then we have
\[
\frac{\partial \mathcal{L}}{\partial y_{k,m}}=\sum_{n=1}^N r_{nk}\frac{2(y_{k,m}-x_{n,m})}{2|x_n-y_k|}
\]

\end{enumerate}
\noindent \textbf{Problem 4.}
\begin{enumerate}
\item[a.] 
\begin{enumerate}
\item[(a)] 
\[
\begin{tabular}{|l|l|}
\hline
k & Mean squared distance \\
\hline
1 &  1.97 \\
2 & 1.80 \\
3 & 1.81 \\
4 &  1.77 \\
5 & 1.52 \\
6 & 1.37 \\
7 & 1.42 \\
8 & 1.25 \\
9 & 1.31 \\
10 & 1.26 \\
\hline
\end{tabular}
\]
\item(b) It is difficult to tell, because the mean squared values always seem to be improving as the number of clusters increases (which should be the case). However, the improvements seem to level off somewhat at 8 clusters, so that is about as reasonable a guess as we can make just from the above data. 
\end{enumerate}


 \item[b.]
\begin{enumerate}
\item[(a)]

Number of instances in each cluster by metric:

 \begin{tabular}{ccc}

  Cluster Number&Min metric &Max metric\\
 0&73&19\\
 1&25&23\\
 2&1&32\\
 3&1&26\\
 \end{tabular}

Scatterplot of Min metric clusters:

\includegraphics[height=2in]{scatter_metric0.png}

Scatterplot of Max metric clusters:

\includegraphics[height=2in]{scatter_metric1.png}

\item[(b)]
Number of instances in each cluster by metric:

 \begin{tabular}{ccc}
  Cluster Number&mean metric &centroid metric\\
 0&152&147\\
 1&46&47\\
 2&1&5\\
 3&1&1\\
 \end{tabular}

Scatterplot of mean metric clusters:

\includegraphics[height=2in]{scatter_metric2.png}

Scatterplot of centroid metric clusters:

\includegraphics[height=2in]{scatter_metric3.png}

\end{enumerate}

\item[c.]
\begin{enumerate}
\item[(a)]
It takes autoclass 18 iterations to converge.
The cluster sizes were 239,394,363 and 4.

\item[(b)]

\includegraphics[height=2in]{log-likelihood.png}

\item[(c)]

Autoclass takes about a minute to classify 1000 instances. K-means takes about a second. Thus K-means has a significantly better run-time than Autoclass.

\end{enumerate}

\end{enumerate}

\end{document}



